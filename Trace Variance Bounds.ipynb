{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A Numerical Solver for the Dual Variance Problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Review of the Problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This Jupyter notebook accompanies Section 3.6 of my PhD Thesis, and provides a method to explicitly compute the dual supremum. \n",
    "In particular, we use the formulas in Proposition 3.20 and Corollary 3.21 to reduce this computation to a finite-dimensional optimization problem over $(\\lambda,\\eta)\\in\\R^2\\times (-\\infty,0).$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, import some libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import scipy.optimize as opt\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=np.ComplexWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we implement Equation (3.15) in Proposition 3.20 of my thesis which, given $x\\in [0,\\infty)\\times (0,\\infty)$ and $(\\lambda,\\eta)\\in \\R^2\\times (-\\infty,0)$, allows us to compute $$Y(x;\\lambda,\\eta)\\in \\operatorname{argmin}_{y\\in[0,\\infty)^2} c(x,y)-\\varphi(x)-\\lambda\\cdot y-\\eta|y-m|^2.$$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start by defining a few auxiliary functions\n",
    "def func_A(x2: float, eta : float) -> float:\n",
    "    return - 4 *  x2 * eta\n",
    "\n",
    "def func_B1(x1 : float, x2: float, lda1 : float, eta : float, m1: float) -> float:\n",
    "    return 2 * x1 + 2 * x2 * lda1 - 4 * x2 * eta * m1\n",
    "\n",
    "def func_B2(x1 : float, x2: float, lda2 : float, eta : float,  m2 : float) -> float:\n",
    "    return x1**2 - 2 * x2 * lda2 + 4 * x2 * eta * m2\n",
    "\n",
    "def func_C(x1 : float, lda1 : float, lda2 : float, eta : float, m1: float, m2 : float) -> float:\n",
    "    return (2 * eta * (m1 * x1 + m2) - (lda1 * x1 +lda2))/(2* eta * (x1**2 +1))\n",
    "\n",
    "# Next, define the cost function\n",
    "def func_c(x1: float, x2: float, y1: float, y2: float) -> float:\n",
    "    if (x2 > 0) and (y2 > 0):\n",
    "        return (y2/(2*x2))*(x1-y1/y2)**2\n",
    "    elif x1 * y2 - y1 == 0:\n",
    "        return 0\n",
    "    else: \n",
    "        return float('inf') \n",
    "\n",
    "# Define the largest solution of $g(u)=u^3+(2-B_2)u-B_1=0$\n",
    "def func_u0(B1: float, B2: float) -> float:\n",
    "    return np.max(np.roots([1, 0, 2-B2, -B1]))\n",
    "\n",
    "# Compute $Y$ as in Proposition 3.20.\n",
    "def func_Y(x1 : float, x2: float, lda1 : float, lda2 : float, eta : float, m1: float, m2 : float) -> np.ndarray:\n",
    "    A = func_A(x2,eta)\n",
    "    B1 = func_B1(x1, x2, lda1, eta, m1)\n",
    "    B2 = func_B2(x1, x2, lda2, eta, m2)\n",
    "    C = func_C(x1, lda1, lda2, eta, m1, m2)\n",
    "    u0 = func_u0(B1,B2)\n",
    "    if (x2 > 0) and (B1 > 0) and (4*B2 < B1**2):\n",
    "        return ((u0**2 - B2)/A) * np.array([u0, 1])\n",
    "    elif (x2 > 0) and (B1 <= 0) and (B2 < 0):\n",
    "        return np.array([0,-B2/A])\n",
    "    elif (x2 == 0) and (C > 0):\n",
    "        return C*np.array([x1,1])\n",
    "    else:\n",
    "        return np.array([0,0])\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we define a measure and compute key quantities. We encode discrete probability measures as `numpy` `ndarrays`, where each row has three entries -- the mass, the $x_1$ coordinate, and the $x_2$ coordinate. We also set a mean target and a variance target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define one sample measure for which the dual supremum appears to be attained\n",
    "mu_denorm_attain = np.array([[1, 1, 1], [1, 2, 1], [1, 1, 2], [1, 2,2]], dtype = 'f')\n",
    "# Define another sample measure for which the dual supremum appears not to be attained. \n",
    "mu_denorm_nonattain = np.array([[1, 2, 4]], dtype = 'f')\n",
    "# Automatically normalize both measures\n",
    "mu_norm_attain = mu_denorm_attain\n",
    "mu_norm_attain[:,0] = mu_norm_attain[:,0]/(np.sum(mu_norm_attain[:,0], axis = 0))\n",
    "mu_norm_nonattain = mu_denorm_nonattain\n",
    "mu_norm_nonattain[:,0] = mu_norm_nonattain[:,0]/(np.sum(mu_norm_nonattain[:,0], axis = 0))\n",
    "\n",
    "# Set mean target\n",
    "m = np.array([1,2])\n",
    "# Set variance target\n",
    "tau = 1.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This choice of $Y,$ along with the presence of a prescribed mean, allows us to compute an optimal value of $\\varphi$ as in Corollary 3.21:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimal_phi(x1: float, x2: float, lda1: float, lda2: float, eta: float) -> float:\n",
    "    y1,y2 = func_Y(x1, x2, lda1, lda2, eta, m[0], m[1])\n",
    "    return func_c(x1, x2, y1, y2) - lda1 * y1 - lda2 * y2 - eta * ((y1-m[0])**2 + (y2-m[1])**2) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we define a function to compute the dual objective function \n",
    "$$\\int \\varphi\\; d\\mu+\\lambda\\cdot y+\\eta\\tau.$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective_function(params, measure) -> float:\n",
    "    lda1, lda2, theta = params\n",
    "    eta = -np.exp(theta) # technical tweak to get an unbounded domain\n",
    "    # negative because scipy can only minimize\n",
    "    return (-sum([measure[i,0] * optimal_phi(measure[i, 1], measure[i, 2], lda1, lda2, eta) for i in range(len(measure))])\n",
    "            - lda1 * m[0] - lda2 * m[1] - eta * tau )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will also wish to compute the quantities $$\\int Y(\\cdot;\\lambda,\\eta)\\; d\\mu$$ and $$\\int |Y(\\cdot;\\lambda,\\eta)-m|^2\\; d\\mu$$ to verify if our result actually achieves the mean and variance constraints:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mu_mean(params, measure) -> np.ndarray:\n",
    "    lda1, lda2, theta = params\n",
    "    eta = -np.exp(theta)\n",
    "    return sum([measure[i,0] * func_Y(measure[i,1], measure[i,2], lda1, lda2, eta, m[0], m[1]) for i in range(len(measure))])\n",
    "\n",
    "def mu_variance(params, measure) -> np.ndarray:\n",
    "    lda1, lda2, theta = params\n",
    "    eta = -np.exp(theta)\n",
    "    return sum([measure[i,0] * np.linalg.norm(func_Y(measure[i,1], measure[i,2], lda1, lda2, eta, m[0], m[1])-m)**2 for i in range(len(measure))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Moreover, we compute $$\\int c(x, Y(x;\\lambda,\\eta))d\\mu(x)$$ in order to get a sense of the value of the primal infimum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "def primal_cost(params, measure) -> np.ndarray:\n",
    "    lda1, lda2, theta = params\n",
    "    eta = -np.exp(theta)\n",
    "    return sum([measure[i,0] * func_c(measure[i,1], measure[i,2], \n",
    "                                      func_Y(measure[i,1], measure[i,2], lda1, lda2, eta, m[0], m[1])[0],\n",
    "                                      func_Y(measure[i,1], measure[i,2], lda1, lda2, eta, m[0], m[1])[1]) \n",
    "                                      for i in range(len(measure))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, for technical reasons, we also create a wrapper so that we can use SciPy's optimize function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_objective(measure):\n",
    "    def wrapped_objective(params):\n",
    "        return objective_function(params, measure)\n",
    "    return wrapped_objective"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we finally complete the optimization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimization_result_attain = opt.minimize(make_objective(mu_norm_attain), [-1,1,-1], method = 'Nelder-Mead')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We recover the optimal choices of $(\\lambda,\\eta).$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lambda = [-0.47282743  0.54774745]\n",
      "eta = -0.09954221634326921\n"
     ]
    }
   ],
   "source": [
    "optimal_params_attain = optimization_result_attain.x\n",
    "print(\"lambda = \" +  str(optimal_params_attain[0:2]))\n",
    "print(\"eta = \" +  str(-np.exp(optimal_params_attain[2])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we verify that the result achieves the target mean and variance, which is the case for `mu_norm_attain`. We also compute the value of the objective function, which in this case should be approximately equal to the dual supremum. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean: [0.99999678+0.j 2.00002156+0.j]\n",
      "variance: 1.5000480053980814\n",
      "dual value: (0.32402810105060625+0j)\n",
      "primal value: (0.3240366522395013+0j)\n"
     ]
    }
   ],
   "source": [
    "print(\"mean: \" +  str(mu_mean(optimal_params_attain, mu_norm_attain)))\n",
    "print(\"variance: \" + str(mu_variance(optimal_params_attain, mu_norm_attain)))\n",
    "print(\"dual value: \" + str(-objective_function(optimal_params_attain, mu_norm_attain)))\n",
    "print(\"primal value: \" + str(primal_cost(optimal_params_attain, mu_norm_attain)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On the other hand, `mu_norm_nonattain` does not meet the target mean and variance, indicating non-attainment of the dual supremum (which we have already seen in Proposition 3.3):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lambda = [-0.375    0.46875]\n",
      "eta = -1.7360330996572665e-16\n",
      "mean: [1.+0.j 2.+0.j]\n",
      "variance: 3.836616882424559e-18\n",
      "dual value: (-0.9374999999999996+0j)\n",
      "primal value: (0.5624999989324116+0j)\n"
     ]
    }
   ],
   "source": [
    "optimization_result_nonattain = opt.minimize(make_objective(mu_norm_nonattain), [-1,1,-1], method = 'Nelder-Mead')\n",
    "optimal_params_nonattain = optimization_result_nonattain.x\n",
    "optimal_params_nonattain[2] = -np.exp(optimal_params_nonattain[2])\n",
    "print(\"lambda = \" +  str(optimal_params_nonattain[0:2]))\n",
    "print(\"eta = \" +  str(optimal_params_nonattain[2]))\n",
    "print(\"mean: \" +  str(mu_mean(optimal_params_nonattain, mu_norm_nonattain)))\n",
    "print(\"variance: \" + str(mu_variance(optimal_params_nonattain, mu_norm_nonattain)))\n",
    "print(\"dual value: \" + str(-objective_function(optimal_params_nonattain, mu_norm_nonattain)))\n",
    "print(\"primal value: \" + str(primal_cost(optimal_params_nonattain, mu_norm_nonattain)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observe that the optimization algorithm yields $\\eta \\approx -1.7\\times 10^{-16}\\approx 0.$ We interpret this to mean that the optimization procedure approaches $\\eta=0,$ but discontinuity of the objective function here prevents the existence of a maximum. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
