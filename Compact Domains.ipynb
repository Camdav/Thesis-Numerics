{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A Numerical Solver for the Compact Domains Problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Review of the Problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This Jupyter notebook accompanies Section 4.5 of my PhD Thesis, and provides a method to explicitly compute the dual supremum $\\mathcal{S}_{X,Y}(\\mu,m)$ and the primal infimum $\\mathcal{I}_{X,Y}(\\mu,m)$ where $Y=[0,R]^2$. It also allows us to compute the dual optimizer $(\\varphi^{\\lambda_0},\\lambda_0).$ Stability issues mean that, in theory, this algorithm may not always identify a primal optimizer $\\gamma_0,$ but it seems to succeed in most contexts.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, import some libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import scipy.optimize as opt\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=np.ComplexWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we implement Equation (4.6) in Proposition 4.13 of my thesis which, given $x\\in [0,\\infty)\\times (0,\\infty),$ $\\lambda\\in \\R^2$, and $R>0$, allows us to compute $$Y(x;\\lambda)\\in \\operatorname{argmin}_{y\\in[0,R]^2} [c(x,y)-\\varphi(x)-\\lambda\\cdot y].$$\n",
    "For the purpose of simplifying the numerics, we assume, in the case where  $\\operatorname{argmin}_{y\\in[0,R]^2} [c(x,y)-\\varphi(x)-\\lambda\\cdot y]$ is a line segment; $Y(x;\\lambda)$ has maximum possible norm (i.e. either $Y_1(x;\\lambda)=R$ or $Y_2(x;\\lambda)=R$). This may lead to overestimating the primal cost, and violation of the mean constraint, but such issues seem rare in practice, and are likely to have only minor impacts for measures whose support has large cardinality. \n",
    "\n",
    "Additionally, the fact that $\\operatorname{argmin}_{y\\in[0,R]^2} [c(x,y)-\\varphi(x)-\\lambda\\cdot y]$ is not a continuous function of $x$ may lead to numerical errors when solving the primal problem. As such, it is important to check that the resulting measure $\\gamma_0$ indeed satisfies the mean constraint, and to compare $\\int c\\; d\\gamma_0$ with the dual supremum. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start by defining the auxiliary functions\n",
    "def func_Lda1(x1 : float, x2: float, lda1 : float) -> float:\n",
    "    return  x1 +  x2 * lda1 \n",
    "\n",
    "def func_Lda2(x1 : float, x2: float, lda2 : float) -> float:\n",
    "    return x1**2 - 2 * x2 * lda2 \n",
    "\n",
    "# Next, define the cost function\n",
    "def func_c(x1: float, x2: float, y1: float, y2: float) -> float:\n",
    "    if (x2 > 0) and (y2 > 0):\n",
    "        return (y2/(2*x2))*(x1-y1/y2)**2\n",
    "    elif x1 * y2 - y1 == 0:\n",
    "        return 0\n",
    "    else: \n",
    "        return float('inf') \n",
    "    \n",
    "# Compute $Y$ as in Equation (4.6) of my thesis.\n",
    "def func_Y(x1 : float, x2: float, lda1 : float, lda2 : float, R : float) -> np.ndarray:\n",
    "    Lda1 = func_Lda1(x1, x2, lda1)\n",
    "    Lda2 = func_Lda2(x1, x2, lda2)\n",
    "    if (Lda1 <= 0) and (Lda2 <= 0):\n",
    "        return np.array([0,R])\n",
    "    elif (0 <= Lda1 <=1) and (Lda2 <= Lda1**2):\n",
    "        return R * np.array([Lda1,1])\n",
    "    elif (Lda2 <= 1 <= Lda1):\n",
    "        return np.array([R,R])\n",
    "    elif (Lda1 >= 1) and (1 <= Lda2 <= Lda1**2):\n",
    "        return R*np.array([1, 1/np.sqrt(Lda2)])\n",
    "    else:\n",
    "        return np.array([0,0])\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we define a measure and compute key quantities. We encode discrete probability measures as `numpy` `ndarrays`, where each row has three entries -- the mass, the $x_1$ coordinate, and the $x_2$ coordinate. We also set a mean target and a variance target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define one sample measure  with a small number (4) of points in its support\n",
    "mu_denorm= np.array([[1, 1, 1], [1, 2, 1], [1, 1, 2], [1, 2,2]], dtype = 'f')\n",
    "# Define another sample measure with a large number (1024) of points in its support\n",
    "mu_spread_denorm = np.ones((1024, 3), dtype='f')\n",
    "mu_spread_denorm[:, 1:] +=  np.random.rand(mu_spread_denorm.shape[0], 2)\n",
    "# Automatically normalize both measures\n",
    "mu_norm = mu_denorm\n",
    "mu_norm[:,0] = mu_norm[:,0]/(np.sum(mu_norm[:,0], axis = 0))\n",
    "mu_spread_norm = mu_spread_denorm\n",
    "mu_spread_norm[:,0] = mu_spread_norm[:,0]/(np.sum(mu_spread_norm[:,0], axis = 0))\n",
    "\n",
    "# Set mean target\n",
    "m = np.array([1.3,1.5])\n",
    "# Set size of $Y$\n",
    "side_length = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The choice of $Y=[0,R]^2$ along with the presence of a prescribed mean, allows us to compute $\\varphi^\\lambda$ (which is the optimal choice of $\\varphi$ admissible for the dual supremum) as in Equation (4.7). This function is continuous in $x$ and $\\lambda$, so should not cause issues with numerical computation of the dual supremum. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def phi_lambda(x1: float, x2: float, lda1: float, lda2: float, R : float) -> float:\n",
    "    Lda1 = func_Lda1(x1, x2, lda1)\n",
    "    Lda2 = func_Lda2(x1, x2, lda2)\n",
    "    if (Lda1 <= 0) and (Lda2 <= 0):\n",
    "        return (R * Lda2) / (2 * x2)\n",
    "    elif (0 <= Lda1 <=1) and (Lda2 <= Lda1**2):\n",
    "        return R*(Lda2 - Lda1**2)/(2 * x2)\n",
    "    elif (Lda2 <= 1 <= Lda1):\n",
    "        return R*(Lda2 - 2 * Lda1 + 1)/(2 * x2)\n",
    "    elif (Lda1 >= 1) and (1 <= Lda2 <= Lda1**2):\n",
    "        return R*(np.sqrt(Lda2) -  Lda1)/(x2)\n",
    "    else:\n",
    "        return 0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we define a function to compute the dual objective function \n",
    "$$\\int \\varphi\\; d\\mu+\\lambda\\cdot y.$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective_function(params, R, measure) -> float:\n",
    "    lda1, lda2 = params\n",
    "    # negative because scipy can only minimize\n",
    "    return (-sum([measure[i,0] * phi_lambda(measure[i, 1], measure[i, 2], lda1, lda2, R) for i in range(len(measure))])\n",
    "            - lda1 * m[0] - lda2 * m[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will also wish to compute the mean $$\\int Y(\\cdot;\\lambda)\\; d\\mu$$ to verify if our result actually achieves the mean constraint:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mu_mean(params, R, measure) -> np.ndarray:\n",
    "    lda1, lda2 = params\n",
    "    return sum([measure[i,0] * func_Y(measure[i,1], measure[i,2], lda1, lda2, R) for i in range(len(measure))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also recover the $(\\Lambda_1,\\Lambda_2)$ for each point in the support of a given $\\mu$; this is useful for debugging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Ldas(measure, lda):\n",
    "    return {tuple(measure[i,1:]) : \n",
    "            [func_Lda1(measure[i,1],measure[i,2], lda[0]),func_Lda2(measure[i,1],measure[i,2], lda[1])] \n",
    "            for i in range(len(measure))}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Moreover, we compute $$\\int c(x, Y(x;\\lambda))d\\mu(x)$$ in order to get a sense of the value of the primal infimum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def primal_cost(params, R, measure) -> np.ndarray:\n",
    "    lda1, lda2 = params\n",
    "    return sum([measure[i,0] * func_c(measure[i,1], measure[i,2], \n",
    "                                      func_Y(measure[i,1], measure[i,2], lda1, lda2, R)[0],\n",
    "                                      func_Y(measure[i,1], measure[i,2], lda1, lda2, R)[1]) \n",
    "                                      for i in range(len(measure))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, for technical reasons, we also create a wrapper so that we can use SciPy's optimize function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_objective(R, measure):\n",
    "    def wrapped_objective(params):\n",
    "        return objective_function(params, R, measure)\n",
    "    return wrapped_objective"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we finally complete the optimization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       message: Optimization terminated successfully.\n",
      "       success: True\n",
      "        status: 0\n",
      "           fun: -0.008888888888888946\n",
      "             x: [-8.889e-02  1.460e-01]\n",
      "           nit: 69\n",
      "          nfev: 127\n",
      " final_simplex: (array([[-8.889e-02,  1.460e-01],\n",
      "                       [-8.889e-02,  1.459e-01],\n",
      "                       [-8.889e-02,  1.460e-01]]), array([-8.889e-03, -8.889e-03, -8.889e-03]))\n"
     ]
    }
   ],
   "source": [
    "optimization_result = opt.minimize(make_objective(side_length, mu_norm), [-1,1], method = 'Nelder-Mead')\n",
    "print(optimization_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We recover the optimal choice of $\\lambda.$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lambda = [-0.08888889  0.14595935]\n"
     ]
    }
   ],
   "source": [
    "optimal_lda = optimization_result.x\n",
    "print(\"lambda = \" +  str(optimal_lda))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we verify that the result achieves the target mean, which is the case for `mu_norm`. We also compute the value of the primal infimum and dual supremum, finding that they are equal in this case. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean: [1.29999999 1.5       ]\n",
      "dual value: 0.008888888888888946\n",
      "primal value: 0.008888889787490745\n"
     ]
    }
   ],
   "source": [
    "print(\"mean: \" +  str(mu_mean(optimal_lda, side_length, mu_norm)))\n",
    "print(\"dual value: \" + str(-objective_function(optimal_lda, side_length, mu_norm)))\n",
    "print(\"primal value: \" + str(primal_cost(optimal_lda, side_length,  mu_norm)))\n",
    "#print(Ldas(mu_norm, optimal_lda))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
