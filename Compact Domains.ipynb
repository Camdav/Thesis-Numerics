{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A Numerical Solver for the Compact Domains Problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Review of the Problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This Jupyter notebook accompanies Section 4.5 of my PhD Thesis, and provides a method to explicitly compute the dual supremum $\\mathcal{S}_{X,Y}(\\mu,m)$ and the primal infimum $\\mathcal{I}_{X,Y}(\\mu,m)$ where $Y=[0,R]^2$. It also allows us to compute the dual optimizer $(\\varphi^{\\lambda_0},\\lambda_0).$ Stability issues mean that, in theory, this algorithm may not always identify a primal optimizer $\\gamma_0,$ but the algorithm becomes more robust if we replace each point of $\\mu$ with a small point cloud.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, import some libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.optimize as opt\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=np.ComplexWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we implement Equation (4.6) in Proposition 4.13 of my thesis which, given $x\\in [0,\\infty)\\times (0,\\infty),$ $\\lambda\\in \\R^2$, and $R>0$, allows us to compute $$Y(x;\\lambda)\\in \\operatorname{argmin}_{y\\in[0,R]^2} [c(x,y)-\\varphi(x)-\\lambda\\cdot y].$$\n",
    "For the purpose of simplifying the numerics, we assume, in the case where  $\\operatorname{argmin}_{y\\in[0,R]^2} [c(x,y)-\\varphi(x)-\\lambda\\cdot y]$ is a line segment, $Y(x;\\lambda)$ has maximum possible norm (i.e. either $Y_1(x;\\lambda)=R$ or $Y_2(x;\\lambda)=R$). This may lead to overestimating the primal cost, and violation of the mean constraint, but such issues seem rare in practice, and are likely to have only minor impacts for measures whose support has large cardinality. \n",
    "\n",
    "Additionally, the fact that $\\operatorname{argmin}_{y\\in[0,R]^2} [c(x,y)-\\varphi(x)-\\lambda\\cdot y]$ is not a continuous function of $x$ may lead to numerical errors when solving the primal problem. As such, it is important to check that the resulting optimal primal measure $\\gamma_0$ indeed satisfies the mean constraint, and to compare $\\int c\\; d\\gamma_0$ with the dual supremum. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start by defining the auxiliary functions from Proposition 4.13\n",
    "def func_Lda1(x1 : float, x2: float, lda1 : float) -> float:\n",
    "    return  x1 +  x2 * lda1 \n",
    "def func_Lda2(x1 : float, x2: float, lda2 : float) -> float:\n",
    "    return x1**2 - 2 * x2 * lda2 \n",
    "\n",
    "# Next, define the cost function from Equation (1.5)\n",
    "def func_c(x1: float, x2: float, y1: float, y2: float) -> float:\n",
    "    if (x2 > 0) and (y2 > 0):\n",
    "        return (y2/(2*x2))*(x1-y1/y2)**2\n",
    "    elif x1 * y2 - y1 == 0:\n",
    "        return 0\n",
    "    else: \n",
    "        return float('inf') \n",
    "    \n",
    "# Compute $Y$ as in Equation (4.6) of my thesis.\n",
    "def func_Y(x1 : float, x2: float, lda1 : float, lda2 : float, R : float) -> np.ndarray:\n",
    "    Lda1 = func_Lda1(x1, x2, lda1)\n",
    "    Lda2 = func_Lda2(x1, x2, lda2)\n",
    "    if (Lda1 <= 0) and (Lda2 <= 0):\n",
    "        return np.array([0,R])\n",
    "    elif (0 <= Lda1 <=1) and (Lda2 <= Lda1**2):\n",
    "        return R * np.array([Lda1,1])\n",
    "    elif (Lda2 <= 1 <= Lda1):\n",
    "        return np.array([R,R])\n",
    "    elif (Lda1 >= 1) and (1 <= Lda2 <= Lda1**2):\n",
    "        return R*np.array([1, 1/np.sqrt(Lda2)])\n",
    "    else:\n",
    "        return np.array([0,0])\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we define a measure and compute key quantities. We encode discrete probability measures as `numpy` `ndarrays`, where each row has three entries -- the mass, the $x_1$ coordinate, and the $x_2$ coordinate. We also set a mean target and a variance target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define one sample measure  with a small number of points in its support\n",
    "mu_denorm= np.array([[1, 1, 1], [1, 2, 1], [1, 1, 2], [1, 2,2]], dtype = 'f')\n",
    "\n",
    "# Automatically normalize\n",
    "mu_norm = mu_denorm\n",
    "mu_norm[:,0] = mu_norm[:,0]/(np.sum(mu_norm[:,0], axis = 0))\n",
    "\n",
    "# Set mean target\n",
    "mean = np.array([1.3,1.5])\n",
    "# Set size of $Y$\n",
    "side_length = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The choice of $Y=[0,R]^2$ along with the presence of a prescribed mean, allows us to compute $\\varphi^\\lambda$ (which is the optimal choice of $\\varphi$ admissible for the dual supremum) as in Equation (4.7). This function is continuous in $x$ and $\\lambda$, so should not cause issues with numerical computation of the dual supremum. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def phi_lambda(x1: float, x2: float, lda1: float, lda2: float, R : float) -> float:\n",
    "    Lda1 = func_Lda1(x1, x2, lda1)\n",
    "    Lda2 = func_Lda2(x1, x2, lda2)\n",
    "    if (Lda1 <= 0) and (Lda2 <= 0):\n",
    "        return (R * Lda2) / (2 * x2)\n",
    "    elif (0 <= Lda1 <=1) and (Lda2 <= Lda1**2):\n",
    "        return R*(Lda2 - Lda1**2)/(2 * x2)\n",
    "    elif (Lda2 <= 1 <= Lda1):\n",
    "        return R*(Lda2 - 2 * Lda1 + 1)/(2 * x2)\n",
    "    elif (Lda1 >= 1) and (1 <= Lda2 <= Lda1**2):\n",
    "        return R*(np.sqrt(Lda2) -  Lda1)/(x2)\n",
    "    else:\n",
    "        return 0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we define a function to compute the dual objective function \n",
    "$$\\int \\varphi\\; d\\mu+\\lambda\\cdot y.$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective_function(params, R, m, measure) -> float:\n",
    "    lda1, lda2 = params\n",
    "    # negative because scipy can only minimize\n",
    "    return (-sum([measure[i,0] * phi_lambda(measure[i, 1], measure[i, 2], lda1, lda2, R) for i in range(len(measure))])\n",
    "            - lda1 * m[0] - lda2 * m[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will also wish to compute the mean $$\\int Y(\\cdot;\\lambda)\\; d\\mu$$ to verify if our result actually achieves the mean constraint:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mu_mean(params, R, measure) -> np.ndarray:\n",
    "    lda1, lda2 = params\n",
    "    return sum([measure[i,0] * func_Y(measure[i,1], measure[i,2], lda1, lda2, R) for i in range(len(measure))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also recover the $(\\Lambda_1,\\Lambda_2)$ for each point in the support of a given $\\mu$; this is useful for debugging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Ldas(measure, lda):\n",
    "    return {tuple(measure[i,1:]) : \n",
    "            [func_Lda1(measure[i,1],measure[i,2], lda[0]),func_Lda2(measure[i,1],measure[i,2], lda[1])] \n",
    "            for i in range(len(measure))}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Moreover, we compute $$\\int c(x, Y(x;\\lambda))d\\mu(x)$$ in order to get a sense of the value of the primal infimum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def primal_cost(params, R, measure) -> np.ndarray:\n",
    "    lda1, lda2 = params\n",
    "    return sum([measure[i,0] * func_c(measure[i,1], measure[i,2], \n",
    "                                      func_Y(measure[i,1], measure[i,2], lda1, lda2, R)[0],\n",
    "                                      func_Y(measure[i,1], measure[i,2], lda1, lda2, R)[1]) \n",
    "                                      for i in range(len(measure))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, for technical reasons, we also create a wrapper so that we can use SciPy's optimize function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_objective(R, m, measure):\n",
    "    def wrapped_objective(params):\n",
    "        return objective_function(params, R, m, measure)\n",
    "    return wrapped_objective"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we finally complete the optimization, using the Nelder-Mead method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       message: Optimization terminated successfully.\n",
      "       success: True\n",
      "        status: 0\n",
      "           fun: -0.008888888888888946\n",
      "             x: [-8.889e-02  1.460e-01]\n",
      "           nit: 69\n",
      "          nfev: 127\n",
      " final_simplex: (array([[-8.889e-02,  1.460e-01],\n",
      "                       [-8.889e-02,  1.459e-01],\n",
      "                       [-8.889e-02,  1.460e-01]]), array([-8.889e-03, -8.889e-03, -8.889e-03]))\n"
     ]
    }
   ],
   "source": [
    "optimization_result = opt.minimize(make_objective(side_length, mean, mu_norm), [-1,1], method = 'Nelder-Mead')\n",
    "print(optimization_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We recover the optimal choice of $\\lambda.$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lambda = [-0.08888889  0.14595935]\n"
     ]
    }
   ],
   "source": [
    "optimal_lda = optimization_result.x\n",
    "print(\"lambda = \" +  str(optimal_lda))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we verify that the result achieves the target mean, which is the case for `mu_norm`. We also compute the value of the primal infimum and dual supremum, finding that they are equal in this case. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean: [1.29999999 1.5       ]\n",
      "dual value: 0.008888888888888946\n",
      "primal value: 0.008888889787490745\n"
     ]
    }
   ],
   "source": [
    "print(\"mean: \" +  str(mu_mean(optimal_lda, side_length, mu_norm)))\n",
    "print(\"dual value: \" + str(-objective_function(optimal_lda, side_length, mean, mu_norm)))\n",
    "print(\"primal value: \" + str(primal_cost(optimal_lda, side_length,  mu_norm)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, changing the mean can lead to errors with the primal infimum, as we see in the following, where $m=(1,2)$ rather than $(1.3, 1.5)$. In particular, we see that the mean constraint is violated, and the primal value dips below the dual value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lambda = [-0.46156262  0.71008519]\n",
      "mean: [0.4614841 1.5      ]\n",
      "dual value: 0.3461538418133254\n",
      "primal value: 0.23967006321533613\n"
     ]
    }
   ],
   "source": [
    "optimization_result_new = opt.minimize(make_objective(side_length, np.array([1,2]), mu_norm), [-1,1], method = 'Nelder-Mead')\n",
    "optimal_lda_new = optimization_result_new.x\n",
    "print(\"lambda = \" +  str(optimal_lda_new))\n",
    "print(\"mean: \" +  str(mu_mean(optimal_lda_new, side_length, mu_norm)))\n",
    "print(\"dual value: \" + str(-objective_function(optimal_lda_new, side_length, np.array([1,2]), mu_norm)))\n",
    "print(\"primal value: \" + str(primal_cost(optimal_lda_new, side_length,  mu_norm)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Upon closer inspection (see below), with the optimal choice of $\\lambda_0\\approx (-0.4616, 0.7101)$, the point $x_0=(2,2)\\in \\operatorname{spt}(\\mu)$ satisfies \n",
    "$$\\Lambda_2(x_0;\\lambda_0)\\approx 1.1597 \\approx(1.0769)^2\\approx(\\Lambda_1(x_0;\\lambda_0))^2,$$\n",
    "meaning that this case is approximately on the threshold between the case $\\Lambda_1\\ge 1$ and $1\\le\\Lambda_2<\\Lambda_1^2$ and the case $\\Lambda_2>0$ and $\\Lambda_1<\\sqrt{\\Lambda_2}$ described in Equation (4.6). In the former case, the mass at $x_0$ would be assigned to $(R, R/\\sqrt{\\Lambda_2})$, and in the latter, the mass at $x_0$ would be assigned to $(0,0)$, which hints at instability in the numerics. In particular, we suspect that the mass at $x_0$ should be assigned to $(R, R/\\sqrt{\\Lambda_2})$ (or somewhere on the line segment between this point and the origin), but small numerical errors lead to this mass being assigned to $(0,0).$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{(1.0, 1.0): [0.5384373756559031, -0.42017037142645286],\n",
       " (2.0, 1.0): [1.538437375655903, 2.579829628573547],\n",
       " (1.0, 2.0): [0.07687475131180632, -1.8403407428529057],\n",
       " (2.0, 2.0): [1.0768747513118062, 1.1596592571470943]}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Ldas(mu_norm, optimal_lda_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To resolve this, we use the following function to replace $\\mu$ with a slightly perturbed version, where each point mass is broken into a cloud of smaller point masses near the original point. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_cloud(mu, K=1024, radius=0.001, rng=None):\n",
    "    if rng is None:\n",
    "        rng = np.random.default_rng(12345)\n",
    "\n",
    "    masses = np.repeat(mu[:, 0] / K, K)\n",
    "    locations = np.repeat(mu[:, 1:], K, axis=0)\n",
    "\n",
    "    noise = rng.uniform(-radius, radius, size=locations.shape)\n",
    "    locations = locations + noise\n",
    "\n",
    "    return np.column_stack([masses, locations])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now run the algorithm with the perturbed version, finding that the optimal $\\lambda$ and dual supremum are largely unchanged, but now the resulting measure (nearly) attains the mean constraint, and the primal infimum is significantly closer to, but still smaller than, the dual supremum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lambda = [-0.46143207  0.71013525]\n",
      "mean: [0.99789762 1.99816643]\n",
      "dual value: 0.346078563744302\n",
      "primal value: 0.3457465905032643\n"
     ]
    }
   ],
   "source": [
    "mu_cloud = make_cloud(mu_norm)\n",
    "optimization_result_cloud = opt.minimize(make_objective(side_length, np.array([1,2]), mu_cloud), [-1,1], method = 'Nelder-Mead')\n",
    "optimal_lda_cloud = optimization_result_cloud.x\n",
    "print(\"lambda = \" +  str(optimal_lda_cloud))\n",
    "print(\"mean: \" +  str(mu_mean(optimal_lda_cloud, side_length, mu_cloud)))\n",
    "print(\"dual value: \" + str(-objective_function(optimal_lda_cloud, side_length, np.array([1,2]), mu_cloud)))\n",
    "print(\"primal value: \" + str(primal_cost(optimal_lda_cloud, side_length,  mu_cloud)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
